{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"../../results/results_tables/\"\n",
    "results_dir = \"../../results/clembench_v1.6/\"\n",
    "gen_results_dir = \"../../results/general_benchmarks/\"\n",
    "results_csv = [\"baseline\", \"D1\",\"D2\", \"D3\", \"D5\", \"D6\", \"D7\"]#, \"DM\"]\n",
    "raw_csv = [\"baseline\", \"D1\",\"D2\", \"D3\",\"D4\", \"D5\", \"D6\", \"D7\", \"D9_top10\"] \n",
    "games = [\"imagegame\", \"privateshared\", \"referencegame\", \"taboo\", \"wordle\", \"wordle_withclue\", \"wordle_withcritic\"]\n",
    "tasks = [\"mmlu\", \"social_iqa\", \"piqa\"]\n",
    "columns = [\"model\"] + tasks\n",
    "\n",
    "# epsiodes referencegame which were *not* seen in the training data\n",
    "ref_training_episodes = {\n",
    "    '0_line_grids_rows': ['episode_25',  'episode_18',  'episode_29',  'episode_7',  'episode_10',  'episode_15'],\n",
    "    '1_line_grids_columns': ['episode_10',  'episode_0',  'episode_15',  'episode_5',  'episode_3',  'episode_22'],'2_diagonal_grids': ['episode_27',  'episode_24',  'episode_5',  'episode_29',  'episode_17',  'episode_8'],\n",
    "    '3_letter_grids': ['episode_8',  'episode_11',  'episode_29',  'episode_22',  'episode_24',  'episode_13'],\n",
    "    '4_shape_grids': ['episode_15',  'episode_24',  'episode_13',  'episode_9',  'episode_17',  'episode_1'],\n",
    "    '5_random_grids': ['episode_28',  'episode_0',  'episode_23',  'episode_17',  'episode_1',  'episode_20']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from bencheval.py\n",
    "def save_clem_table(df: pd.DataFrame) -> None:\n",
    "    \"\"\"Create benchmark results as a table.\"\"\"\n",
    "    #########\n",
    "    df_aux = df[df['metric'].isin([\"Played\", \"Main Score\"])]\n",
    "\n",
    "    # compute mean benchscore and mean played (which is binary, so a proportion)\n",
    "    df_a = (df_aux.groupby(['game', 'model', 'metric'])\n",
    "                  .mean(numeric_only=True)\n",
    "                  .reset_index())\n",
    "    df_a.loc[df_a.metric == \"Played\", 'value'] *= 100\n",
    "    df_a = df_a.round(2)\n",
    "    df_a['metric'].replace(\n",
    "        {\"Played\": '% Played'},\n",
    "        inplace=True)\n",
    "\n",
    "    # compute the std of benchscore\n",
    "    df_aux_b = df_aux[df_aux.metric == \"Main Score\"]\n",
    "    df_b = (df_aux_b.groupby(['game', 'model', 'metric'])\n",
    "                    .std(numeric_only=True)\n",
    "                    .reset_index()\n",
    "                    .round(2))\n",
    "    df_b['metric'].replace(\n",
    "        {\"Main Score\": \"Main Score\"+' (std)'},\n",
    "        inplace=True)\n",
    "\n",
    "    # compute the macro-average main score over games, per model\n",
    "    df_all = (df_a.groupby(['model', 'metric'])\n",
    "                  .mean(numeric_only=True)\n",
    "                  .reset_index()\n",
    "                  .round(2))\n",
    "    # add columns for standard format in concatenation below\n",
    "    df_all['game'] = 'all'\n",
    "    df_all['metric'] = 'Average ' + df_all['metric']\n",
    "\n",
    "    # merge all data and make it one model per row\n",
    "    df_full = pd.concat([df_a, df_b, df_all], axis=0, ignore_index=True)\n",
    "    # sort just so all metrics are close to each other in a game column\n",
    "    df_full.sort_values(by=['game', 'metric'], inplace=True)\n",
    "    # rename according to paper\n",
    "    df_full['metric'] = df_full['metric'].str.replace(\"Main Score\", 'Quality Score')\n",
    "    df_full = df_full.pivot(columns=['game', 'metric'], index=['model'])\n",
    "    df_full = df_full.droplevel(0, axis=1)\n",
    "\n",
    "    # compute clemscores and add to df\n",
    "    clemscore = ((df_full[('all', 'Average % Played')] / 100)\n",
    "                 * df_full[('all', 'Average Quality Score')])\n",
    "    clemscore = clemscore.round(2).to_frame(name=('-', 'clemscore'))\n",
    "    df_results = pd.concat([clemscore, df_full], axis=1)\n",
    "\n",
    "    # flatten header\n",
    "    df_results.index.name = None\n",
    "    df_results.columns = df_results.columns.to_flat_index() \n",
    "    df_results.columns = [', '.join(x) for x in df_results.columns]\n",
    "\n",
    "    # save table\n",
    "    #df_results.to_csv(Path(path) / f'{TABLE_NAME}.csv')\n",
    "    #df_results.to_html(Path(path) / f'{TABLE_NAME}.html')\n",
    "    #print(f'\\n Saved results into {path}/{TABLE_NAME}.csv and .html')\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove referencegame instances that appeared in the training data (ref_training_episodes)\n",
    "def remove_referencegame_instances(df: pd.DataFrame, ref_training_episodes: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a results table like results.csv excluding the specified referencegame episodes (that have been part of the training data) from raw.csv.\n",
    "\n",
    "    \"\"\"\n",
    "    ind_to_remove = []\n",
    "    for experiment, episodes in ref_training_episodes.items():\n",
    "        for episode in episodes:\n",
    "            ind = df[((df.experiment == experiment) & (df.episode == episode))].index\n",
    "            ind_to_remove += list(ind)\n",
    "\n",
    "    ref_keep = df.loc[ind_to_remove]\n",
    "    df1_no_reftraining = pd.concat([df[~(df.game == \"referencegame\")], ref_keep])\n",
    "    #\n",
    "    # print(\"Results Top10 without referencegame episodes from training (final clemscore?)\")\n",
    "    #full final results\n",
    "    ffr = save_clem_table(df1_no_reftraining)\n",
    "    ffr[[\"-, clemscore\", \"all, Average % Played\", \"all, Average Quality Score\"]]\n",
    "    return ffr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prettify_df(df):\n",
    "    #exclude cols\n",
    "    df = df.loc[:,~df.columns.str.contains('std', case=False)]\n",
    "    df = (df.round(decimals=2)\n",
    "          .sort_values(by = \"short_name\")\n",
    "          )\n",
    "    # shorten model names\n",
    "    if \"--\" in df.index:\n",
    "        short_names = [x.split(\"--\")[0][:-5] for x in list(df.index)]\n",
    "        short_dict = dict(zip(list(df.index), short_names))\n",
    "        df = df.rename(index=short_dict)\n",
    "    return df\n",
    "\n",
    "def save_as_csv_and_tex(df, experiment, save_dir):\n",
    "    df.to_csv(save_dir + \"results_\" + experiment + \".csv\")\n",
    "    df.to_latex(save_dir + \"results_\" + experiment + \".tex\", \n",
    "            float_format=\"%.2f\",\n",
    "            escape = True,\n",
    "            caption = \"Results on experiment \" + experiment + \".\", \n",
    "            label = \"tab:results-\" + experiment,\n",
    "            position= \"h!\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare result tables including general benchmarks (if existing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_short_name = r\"D\\d{5}_*\\w*|llama-3.1\"\n",
    "\n",
    "# all_results = []\n",
    "\n",
    "for experiment in raw_csv:\n",
    "    # read raw\n",
    "    df_raw = pd.read_csv(results_dir + experiment + \"/raw.csv\", index_col = 0)\n",
    "    df_raw = df_raw[df_raw.game.isin(games)] # exclude matchit_ascii!\n",
    "    \n",
    "    df = remove_referencegame_instances(df_raw, ref_training_episodes)\n",
    "\n",
    "    short_names = [re.search(pattern_short_name, name).group() for name in df.index]\n",
    "    df[\"short_name\"] = short_names\n",
    "\n",
    "    results = []\n",
    "    double_but_ok = 0\n",
    "    double_bad = 0\n",
    "    # create general results\n",
    "    for directory in os.listdir(gen_results_dir):   # single model directories\n",
    "        directory = gen_results_dir + directory\n",
    "        if any(name in directory for name in short_names): # is there general benchmark data for this experiment?\n",
    "            if os.path.isdir(directory):\n",
    "                model_results = dict.fromkeys(columns)\n",
    "                model_results[\"model\"] = directory\n",
    "                subfiles = os.listdir(directory)\n",
    "                # pr√ºfen, dass das eigentlich files sind und json!\n",
    "                for file in subfiles:\n",
    "                    file = directory + \"/\" + file\n",
    "                    with open(file) as f:\n",
    "                        jsonfile = json.load(f)\n",
    "                    for task in tasks:\n",
    "                        if task in jsonfile[\"results\"]:\n",
    "                            accuracy = jsonfile[\"results\"][task][\"acc,none\"]\n",
    "                            if model_results[task] is None:\n",
    "                                model_results[task] = accuracy\n",
    "                            else:\n",
    "                                if accuracy == model_results[task]:\n",
    "                                    double_but_ok += 1\n",
    "                                else:\n",
    "                                    #print(\"ATTENTION FOR\", directory)\n",
    "                                    #print(task)\n",
    "                                    #print(\"Trying to set different\")\n",
    "                                    double_bad += 1\n",
    "                results.append(model_results)\n",
    "\n",
    "\n",
    "    try:    \n",
    "        results_df = pd.concat([pd.DataFrame(x, index = [0]) for x in results]).reset_index()\n",
    "        results_df[\"short_name\"] = [re.search(pattern_short_name, name).group() for name in results_df.model]\n",
    "        results_df[tasks]*=100\n",
    "        final_df = pd.merge(results_df[[\"short_name\"] + tasks], df, \n",
    "                        how = \"outer\",\n",
    "                        on = \"short_name\",\n",
    "                        ).set_index(\"short_name\")\n",
    "    except ValueError:\n",
    "        print(\"No general benchmark files for \" + experiment)\n",
    "        final_df = df\n",
    "    \n",
    "    # all_results.append(final_df)\n",
    "      \n",
    "    save_as_csv_and_tex(prettify_df(final_df), experiment, save_dir= save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment only if new table will not be committed or changes for google sheets will be done befor (i.e. putting llama in first row)\n",
    "# save_as_csv_and_tex(prettify_df(pd.concat(all_results)),\"all\", save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Prepare result tables without general benchmarks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare simple results tables\n",
    "for experiment in results_csv:\n",
    "    df = pd.read_csv(results_dir +  experiment + \"/results.csv\", index_col=0)\n",
    "\n",
    "    #TODO: only keep games in game list (i.e. exclude matchit_ascii to not be part of the evaluation)\n",
    "    #exclude cols\n",
    "    df = df.loc[:,~df.columns.str.contains('std', case=False)]\n",
    "    # shorten model names\n",
    "    short_names = [x.split(\"--\")[0][:-5] for x in list(df.index)]\n",
    "    short_dict = dict(zip(list(df.index), short_names))\n",
    "    df = df.rename(index=short_dict)\n",
    "\n",
    "    \n",
    "# prepare results tables for experiment which need referencegame episodes removed\n",
    "for experiment in raw_csv:\n",
    "    df = pd.read_csv(results_dir + experiment + \"/raw.csv\", index_col = 0)\n",
    "    #df[\"full_inst_name\"] = df[\"experiment\"].str[2:] + \"_\" + df[\"episode\"]\n",
    "    df = remove_referencegame_instances(df, ref_training_episodes)\n",
    "\n",
    "    # rename_dict = {}\n",
    "    # for name1 in results_df.model: \n",
    "    #     for name2 in df.index:\n",
    "    #         if name1 in name2:\n",
    "    #             print(name1)\n",
    "    #             rename_dict[name1] = name2\n",
    "\n",
    "    # results_df = results_df.replace(rename_dict)\n",
    "\n",
    "    # df = pd.merge(results_df, df, right_index=True, left_on = \"model\", how = 'inner')\n",
    "    # df = df.drop(columns=[\"index\"], axis=1)\n",
    "    # print(df.head(n= 1))\n",
    "    # df.to_csv(save_dir + \"results_tables/results_\" + experiment + \".csv\",\n",
    "    #           index = False)\n",
    "    df = df.drop(columns=[\"matchit_ascii, % Played\", \"matchit_ascii, Quality Score\"], axis=1, errors = \"ignore\")\n",
    "    df.to_csv(save_dir + \"results_tables/results_\" + experiment + \".csv\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include general benchmarks playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48311156601842375\n",
      "0.6589517162797323\n",
      "0.7927094668117519\n"
     ]
    }
   ],
   "source": [
    "directory = \"../../results/general_benchmarks/unsloth__meta-llama-3.1-8b-instruct-bnb-4bit\"\n",
    "subfiles = os.listdir(directory)\n",
    "        # pr√ºfen, dass das eigentlich files sind und json!\n",
    "for file in subfiles:\n",
    "    file = directory + \"/\" + file    \n",
    "    with open(file) as f:\n",
    "        jsonfile = json.load(f)\n",
    "    for task in tasks:\n",
    "        if task in jsonfile[\"results\"]:\n",
    "            accuracy = jsonfile[\"results\"][task][\"acc,none\"]\n",
    "            print(accuracy)\n",
    "            \n",
    "        if model_results[task] is None:\n",
    "            model_results[task] = accuracy\n",
    "        else:\n",
    "            if accuracy == model_results[task]:\n",
    "                double_but_ok += 1\n",
    "            else:\n",
    "                #print(\"ATTENTION FOR\", directory)\n",
    "                #print(task)\n",
    "                #print(\"Trying to set different\")\n",
    "                double_bad += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HIER\n",
    "\n",
    "pattern_short_name = r\"D\\d{5}_*\\w*|llama-3.1\"\n",
    "\n",
    "for experiment in raw_csv:\n",
    "    # read raw\n",
    "    df_raw = pd.read_csv(results_dir + experiment + \"/raw.csv\", index_col = 0)\n",
    "    df_raw = df_raw[df_raw.game.isin(games)] # exclude matchit_ascii!\n",
    "    \n",
    "    df = remove_referencegame_instances(df_raw, ref_training_episodes)\n",
    "\n",
    "    short_names = [re.search(pattern_short_name, name).group() for name in df.index]\n",
    "    df[\"short_name\"] = short_names\n",
    "\n",
    "    results = []\n",
    "    double_but_ok = 0\n",
    "    double_bad = 0\n",
    "    # create general results\n",
    "    for directory in os.listdir(gen_results_dir):   # single model directories\n",
    "        directory = gen_results_dir + directory\n",
    "        if any(name in directory for name in short_names): # is there general benchmark data for this experiment?\n",
    "            if os.path.isdir(directory):\n",
    "                model_results = dict.fromkeys(columns)\n",
    "                model_results[\"model\"] = directory\n",
    "                subfiles = os.listdir(directory)\n",
    "                # pr√ºfen, dass das eigentlich files sind und json!\n",
    "                for file in subfiles:\n",
    "                    file = directory + \"/\" + file\n",
    "                    with open(file) as f:\n",
    "                        jsonfile = json.load(f)\n",
    "                    for task in tasks:\n",
    "                        if task in jsonfile[\"results\"]:\n",
    "                            accuracy = jsonfile[\"results\"][task][\"acc,none\"]\n",
    "                            if model_results[task] is None:\n",
    "                                model_results[task] = accuracy\n",
    "                            else:\n",
    "                                if accuracy == model_results[task]:\n",
    "                                    double_but_ok += 1\n",
    "                                else:\n",
    "                                    #print(\"ATTENTION FOR\", directory)\n",
    "                                    #print(task)\n",
    "                                    #print(\"Trying to set different\")\n",
    "                                    double_bad += 1\n",
    "                results.append(model_results)\n",
    "\n",
    "\n",
    "    try:    \n",
    "        results_df = pd.concat([pd.DataFrame(x, index = [0]) for x in results]).reset_index()\n",
    "        results_df[\"short_name\"] = [re.search(pattern_short_name, name).group() for name in results_df.model]\n",
    "        results_df[tasks]*=100\n",
    "        final_df = pd.merge(results_df[[\"short_name\"] + tasks], df, \n",
    "                        on = \"short_name\",\n",
    "                        ).set_index(\"short_name\")\n",
    "    except ValueError:\n",
    "        print(\"No general benchmark files for \" + experiment)\n",
    "        final_df = df\n",
    "            \n",
    "    save_as_csv_and_tex(prettify_df(final_df), experiment, save_dir= save_dir)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>model</th>\n",
       "      <th>mmlu</th>\n",
       "      <th>social_iqa</th>\n",
       "      <th>piqa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>../../results/general_benchmarks/clembench-pla...</td>\n",
       "      <td>0.496226</td>\n",
       "      <td>0.493347</td>\n",
       "      <td>0.761153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>../../results/general_benchmarks/clembench-pla...</td>\n",
       "      <td>0.517092</td>\n",
       "      <td>0.494371</td>\n",
       "      <td>0.762786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>../../results/general_benchmarks/clembench-pla...</td>\n",
       "      <td>0.549210</td>\n",
       "      <td>0.485670</td>\n",
       "      <td>0.773667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>../../results/general_benchmarks/clembench-pla...</td>\n",
       "      <td>0.589588</td>\n",
       "      <td>0.484647</td>\n",
       "      <td>0.781284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>../../results/general_benchmarks/clembench-pla...</td>\n",
       "      <td>0.598490</td>\n",
       "      <td>0.490788</td>\n",
       "      <td>0.792165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>../../results/general_benchmarks/clembench-pla...</td>\n",
       "      <td>0.603404</td>\n",
       "      <td>0.494882</td>\n",
       "      <td>0.779652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>../../results/general_benchmarks/clembench-pla...</td>\n",
       "      <td>0.625196</td>\n",
       "      <td>0.490788</td>\n",
       "      <td>0.788357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>../../results/general_benchmarks/clembench-pla...</td>\n",
       "      <td>0.627475</td>\n",
       "      <td>0.627475</td>\n",
       "      <td>0.627475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>../../results/general_benchmarks/clembench-pla...</td>\n",
       "      <td>0.644068</td>\n",
       "      <td>0.644068</td>\n",
       "      <td>0.644068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>../../results/general_benchmarks/clembench-pla...</td>\n",
       "      <td>0.644353</td>\n",
       "      <td>0.479529</td>\n",
       "      <td>0.789989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0</td>\n",
       "      <td>../../results/general_benchmarks/clembench-pla...</td>\n",
       "      <td>0.645421</td>\n",
       "      <td>0.493347</td>\n",
       "      <td>0.790533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>../../results/general_benchmarks/clembench-pla...</td>\n",
       "      <td>0.646703</td>\n",
       "      <td>0.646703</td>\n",
       "      <td>0.646703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>../../results/general_benchmarks/clembench-pla...</td>\n",
       "      <td>0.647201</td>\n",
       "      <td>0.470317</td>\n",
       "      <td>0.792709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>../../results/general_benchmarks/clembench-pla...</td>\n",
       "      <td>0.647344</td>\n",
       "      <td>0.468270</td>\n",
       "      <td>0.789445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>../../results/general_benchmarks/clembench-pla...</td>\n",
       "      <td>0.647344</td>\n",
       "      <td>0.482600</td>\n",
       "      <td>0.787813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>../../results/general_benchmarks/clembench-pla...</td>\n",
       "      <td>0.647486</td>\n",
       "      <td>0.474923</td>\n",
       "      <td>0.790533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0</td>\n",
       "      <td>../../results/general_benchmarks/clembench-pla...</td>\n",
       "      <td>0.647557</td>\n",
       "      <td>0.490788</td>\n",
       "      <td>0.787813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>../../results/general_benchmarks/clembench-pla...</td>\n",
       "      <td>0.647771</td>\n",
       "      <td>0.647771</td>\n",
       "      <td>0.647771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>../../results/general_benchmarks/clembench-pla...</td>\n",
       "      <td>0.648127</td>\n",
       "      <td>0.491300</td>\n",
       "      <td>0.793798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>../../results/general_benchmarks/clembench-pla...</td>\n",
       "      <td>0.648198</td>\n",
       "      <td>0.473388</td>\n",
       "      <td>0.785637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>../../results/general_benchmarks/clembench-pla...</td>\n",
       "      <td>0.648341</td>\n",
       "      <td>0.462129</td>\n",
       "      <td>0.784548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0</td>\n",
       "      <td>../../results/general_benchmarks/clembench-pla...</td>\n",
       "      <td>0.649338</td>\n",
       "      <td>0.472364</td>\n",
       "      <td>0.786181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0</td>\n",
       "      <td>../../results/general_benchmarks/clembench-pla...</td>\n",
       "      <td>0.649623</td>\n",
       "      <td>0.469294</td>\n",
       "      <td>0.789989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "      <td>../../results/general_benchmarks/clembench-pla...</td>\n",
       "      <td>0.649623</td>\n",
       "      <td>0.480041</td>\n",
       "      <td>0.795974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>../../results/general_benchmarks/clembench-pla...</td>\n",
       "      <td>0.649623</td>\n",
       "      <td>0.470829</td>\n",
       "      <td>0.790533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "      <td>../../results/general_benchmarks/clembench-pla...</td>\n",
       "      <td>0.650762</td>\n",
       "      <td>0.488229</td>\n",
       "      <td>0.787813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0</td>\n",
       "      <td>../../results/general_benchmarks/clembench-pla...</td>\n",
       "      <td>0.652115</td>\n",
       "      <td>0.477994</td>\n",
       "      <td>0.792165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0</td>\n",
       "      <td>../../results/general_benchmarks/clembench-pla...</td>\n",
       "      <td>0.652400</td>\n",
       "      <td>0.467247</td>\n",
       "      <td>0.785092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>../../results/general_benchmarks/clembench-pla...</td>\n",
       "      <td>0.652542</td>\n",
       "      <td>0.482600</td>\n",
       "      <td>0.786725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0</td>\n",
       "      <td>../../results/general_benchmarks/clembench-pla...</td>\n",
       "      <td>0.653255</td>\n",
       "      <td>0.653255</td>\n",
       "      <td>0.653255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0</td>\n",
       "      <td>../../results/general_benchmarks/clembench-pla...</td>\n",
       "      <td>0.654323</td>\n",
       "      <td>0.481064</td>\n",
       "      <td>0.792165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0</td>\n",
       "      <td>../../results/general_benchmarks/clembench-pla...</td>\n",
       "      <td>0.654465</td>\n",
       "      <td>0.479529</td>\n",
       "      <td>0.791621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>../../results/general_benchmarks/clembench-pla...</td>\n",
       "      <td>0.654679</td>\n",
       "      <td>0.488741</td>\n",
       "      <td>0.789989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>../../results/general_benchmarks/clembench-pla...</td>\n",
       "      <td>0.654679</td>\n",
       "      <td>0.486694</td>\n",
       "      <td>0.794342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>../../results/general_benchmarks/clembench-pla...</td>\n",
       "      <td>0.655177</td>\n",
       "      <td>0.655177</td>\n",
       "      <td>0.655177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0</td>\n",
       "      <td>../../results/general_benchmarks/clembench-pla...</td>\n",
       "      <td>0.655391</td>\n",
       "      <td>0.655391</td>\n",
       "      <td>0.655391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>../../results/general_benchmarks/clembench-pla...</td>\n",
       "      <td>0.655605</td>\n",
       "      <td>0.486182</td>\n",
       "      <td>0.792165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>../../results/general_benchmarks/clembench-pla...</td>\n",
       "      <td>0.659806</td>\n",
       "      <td>0.482088</td>\n",
       "      <td>0.790533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>../../results/general_benchmarks/clembench-pla...</td>\n",
       "      <td>0.779652</td>\n",
       "      <td>0.779652</td>\n",
       "      <td>0.795430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0</td>\n",
       "      <td>../../results/general_benchmarks/clembench-pla...</td>\n",
       "      <td>0.786181</td>\n",
       "      <td>0.786181</td>\n",
       "      <td>0.799238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "      <td>../../results/general_benchmarks/unsloth__meta...</td>\n",
       "      <td>0.787813</td>\n",
       "      <td>0.787813</td>\n",
       "      <td>0.787813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>../../results/general_benchmarks/clembench-pla...</td>\n",
       "      <td>0.789445</td>\n",
       "      <td>0.789445</td>\n",
       "      <td>0.789445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>../../results/general_benchmarks/clembench-pla...</td>\n",
       "      <td>0.789989</td>\n",
       "      <td>0.493859</td>\n",
       "      <td>0.493859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>../../results/general_benchmarks/clembench-pla...</td>\n",
       "      <td>0.790533</td>\n",
       "      <td>0.790533</td>\n",
       "      <td>0.786725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0</td>\n",
       "      <td>../../results/general_benchmarks/clembench-pla...</td>\n",
       "      <td>0.791621</td>\n",
       "      <td>0.791621</td>\n",
       "      <td>0.787269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>../../results/general_benchmarks/clembench-pla...</td>\n",
       "      <td>0.792165</td>\n",
       "      <td>0.494882</td>\n",
       "      <td>0.494882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>../../results/general_benchmarks/clembench-pla...</td>\n",
       "      <td>0.793254</td>\n",
       "      <td>0.490276</td>\n",
       "      <td>0.490276</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index                                              model      mmlu   \n",
       "9       0  ../../results/general_benchmarks/clembench-pla...  0.496226  \\\n",
       "18      0  ../../results/general_benchmarks/clembench-pla...  0.517092   \n",
       "6       0  ../../results/general_benchmarks/clembench-pla...  0.549210   \n",
       "31      0  ../../results/general_benchmarks/clembench-pla...  0.589588   \n",
       "1       0  ../../results/general_benchmarks/clembench-pla...  0.598490   \n",
       "12      0  ../../results/general_benchmarks/clembench-pla...  0.603404   \n",
       "21      0  ../../results/general_benchmarks/clembench-pla...  0.625196   \n",
       "0       0  ../../results/general_benchmarks/clembench-pla...  0.627475   \n",
       "20      0  ../../results/general_benchmarks/clembench-pla...  0.644068   \n",
       "15      0  ../../results/general_benchmarks/clembench-pla...  0.644353   \n",
       "35      0  ../../results/general_benchmarks/clembench-pla...  0.645421   \n",
       "26      0  ../../results/general_benchmarks/clembench-pla...  0.646703   \n",
       "7       0  ../../results/general_benchmarks/clembench-pla...  0.647201   \n",
       "28      0  ../../results/general_benchmarks/clembench-pla...  0.647344   \n",
       "17      0  ../../results/general_benchmarks/clembench-pla...  0.647344   \n",
       "25      0  ../../results/general_benchmarks/clembench-pla...  0.647486   \n",
       "39      0  ../../results/general_benchmarks/clembench-pla...  0.647557   \n",
       "24      0  ../../results/general_benchmarks/clembench-pla...  0.647771   \n",
       "30      0  ../../results/general_benchmarks/clembench-pla...  0.648127   \n",
       "8       0  ../../results/general_benchmarks/clembench-pla...  0.648198   \n",
       "3       0  ../../results/general_benchmarks/clembench-pla...  0.648341   \n",
       "36      0  ../../results/general_benchmarks/clembench-pla...  0.649338   \n",
       "43      0  ../../results/general_benchmarks/clembench-pla...  0.649623   \n",
       "34      0  ../../results/general_benchmarks/clembench-pla...  0.649623   \n",
       "4       0  ../../results/general_benchmarks/clembench-pla...  0.649623   \n",
       "32      0  ../../results/general_benchmarks/clembench-pla...  0.650762   \n",
       "46      0  ../../results/general_benchmarks/clembench-pla...  0.652115   \n",
       "40      0  ../../results/general_benchmarks/clembench-pla...  0.652400   \n",
       "19      0  ../../results/general_benchmarks/clembench-pla...  0.652542   \n",
       "42      0  ../../results/general_benchmarks/clembench-pla...  0.653255   \n",
       "38      0  ../../results/general_benchmarks/clembench-pla...  0.654323   \n",
       "44      0  ../../results/general_benchmarks/clembench-pla...  0.654465   \n",
       "23      0  ../../results/general_benchmarks/clembench-pla...  0.654679   \n",
       "14      0  ../../results/general_benchmarks/clembench-pla...  0.654679   \n",
       "11      0  ../../results/general_benchmarks/clembench-pla...  0.655177   \n",
       "41      0  ../../results/general_benchmarks/clembench-pla...  0.655391   \n",
       "27      0  ../../results/general_benchmarks/clembench-pla...  0.655605   \n",
       "10      0  ../../results/general_benchmarks/clembench-pla...  0.659806   \n",
       "13      0  ../../results/general_benchmarks/clembench-pla...  0.779652   \n",
       "37      0  ../../results/general_benchmarks/clembench-pla...  0.786181   \n",
       "33      0  ../../results/general_benchmarks/unsloth__meta...  0.787813   \n",
       "29      0  ../../results/general_benchmarks/clembench-pla...  0.789445   \n",
       "16      0  ../../results/general_benchmarks/clembench-pla...  0.789989   \n",
       "5       0  ../../results/general_benchmarks/clembench-pla...  0.790533   \n",
       "45      0  ../../results/general_benchmarks/clembench-pla...  0.791621   \n",
       "2       0  ../../results/general_benchmarks/clembench-pla...  0.792165   \n",
       "22      0  ../../results/general_benchmarks/clembench-pla...  0.793254   \n",
       "\n",
       "    social_iqa      piqa  \n",
       "9     0.493347  0.761153  \n",
       "18    0.494371  0.762786  \n",
       "6     0.485670  0.773667  \n",
       "31    0.484647  0.781284  \n",
       "1     0.490788  0.792165  \n",
       "12    0.494882  0.779652  \n",
       "21    0.490788  0.788357  \n",
       "0     0.627475  0.627475  \n",
       "20    0.644068  0.644068  \n",
       "15    0.479529  0.789989  \n",
       "35    0.493347  0.790533  \n",
       "26    0.646703  0.646703  \n",
       "7     0.470317  0.792709  \n",
       "28    0.468270  0.789445  \n",
       "17    0.482600  0.787813  \n",
       "25    0.474923  0.790533  \n",
       "39    0.490788  0.787813  \n",
       "24    0.647771  0.647771  \n",
       "30    0.491300  0.793798  \n",
       "8     0.473388  0.785637  \n",
       "3     0.462129  0.784548  \n",
       "36    0.472364  0.786181  \n",
       "43    0.469294  0.789989  \n",
       "34    0.480041  0.795974  \n",
       "4     0.470829  0.790533  \n",
       "32    0.488229  0.787813  \n",
       "46    0.477994  0.792165  \n",
       "40    0.467247  0.785092  \n",
       "19    0.482600  0.786725  \n",
       "42    0.653255  0.653255  \n",
       "38    0.481064  0.792165  \n",
       "44    0.479529  0.791621  \n",
       "23    0.488741  0.789989  \n",
       "14    0.486694  0.794342  \n",
       "11    0.655177  0.655177  \n",
       "41    0.655391  0.655391  \n",
       "27    0.486182  0.792165  \n",
       "10    0.482088  0.790533  \n",
       "13    0.779652  0.795430  \n",
       "37    0.786181  0.799238  \n",
       "33    0.787813  0.787813  \n",
       "29    0.789445  0.789445  \n",
       "16    0.493859  0.493859  \n",
       "5     0.790533  0.786725  \n",
       "45    0.791621  0.787269  \n",
       "2     0.494882  0.494882  \n",
       "22    0.490276  0.490276  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.concat([pd.DataFrame(x, index = [0]) for x in results]).reset_index()\n",
    "#results_df[\"model\"] = results_df.model.str[94:]\n",
    "results_df.sort_values(\"mmlu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.concat([pd.DataFrame(x, index = [0]) for x in results]).reset_index()\n",
    "results_df[\"model\"] = results_df.model.str[94:]\n",
    "\n",
    "\n",
    "df = pd.read_csv(results_dir + \"D4\" + \"/raw.csv\", index_col = 0)\n",
    "df = remove_referencegame_instances(df, ref_training_episodes)\n",
    "df = df.loc[:,~df.columns.str.contains('std', case=False)]\n",
    "short_names = [x.split(\"--\")[0][:-5] for x in list(df.index)]\n",
    "short_dict = dict(zip(list(df.index), short_names))\n",
    "df = df.rename(index=short_dict)\n",
    "rename_dict = {}\n",
    "for name1 in results_df.model: \n",
    "    for name2 in df.index:\n",
    "        if name1 in name2:\n",
    "            print(name1)\n",
    "            rename_dict[name1] = name2\n",
    "\n",
    "results_df = results_df.replace(rename_dict)\n",
    "\n",
    "df = pd.merge(results_df, df, right_index=True, left_on = \"model\", how = 'inner')\n",
    "df = df.drop(columns=[\"index\"], axis=1)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
