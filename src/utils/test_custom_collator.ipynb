{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "import os\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "df = pd.read_csv('../../data/training_data/DFINAL_VTrain.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = \"<|finetune_right_pad_id|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecialTokenCollator(DataCollatorForLanguageModeling):\n",
    "    def __init__(self, assistant_header, eot_token, tokenizer, *args, **kwargs):\n",
    "        super().__init__(tokenizer=tokenizer, *args, **kwargs)\n",
    "        self.assistant_header = tokenizer.encode(\n",
    "            assistant_header, add_special_tokens=False\n",
    "        )\n",
    "        self.eot_token = tokenizer.encode(eot_token, add_special_tokens=False)[0]\n",
    "        \n",
    "    def torch_call(self, examples):\n",
    "        batch = super().torch_call(examples)\n",
    "        \n",
    "        for i, input_ids in enumerate(batch[\"input_ids\"]):\n",
    "            # Find all assistant headers\n",
    "            header_positions = []\n",
    "            curr_idx = 0\n",
    "            while curr_idx <= len(input_ids) - len(self.assistant_header):\n",
    "                if input_ids[curr_idx:curr_idx+len(self.assistant_header)].tolist() == self.assistant_header:\n",
    "                    header_positions.append(curr_idx)\n",
    "                    curr_idx += len(self.assistant_header)\n",
    "                else:\n",
    "                    curr_idx += 1\n",
    "            \n",
    "            # Create mask for assistant responses\n",
    "            if header_positions:\n",
    "                loss_mask = torch.zeros_like(input_ids)\n",
    "                for pos in header_positions:\n",
    "                    start = pos + len(self.assistant_header)\n",
    "                    end = self.find_eot(input_ids, start)\n",
    "                    loss_mask[start:end] = 1\n",
    "                \n",
    "                batch[\"labels\"][i] = torch.where(\n",
    "                    loss_mask.bool(),\n",
    "                    batch[\"labels\"][i],\n",
    "                    -100\n",
    "                )\n",
    "                \n",
    "        return batch\n",
    "    \n",
    "    def find_eot(self, input_ids, start_idx):\n",
    "        eot_positions = (input_ids[start_idx:] == self.eot_token).nonzero()\n",
    "        if eot_positions.numel() > 0:\n",
    "            eot_pos = start_idx + eot_positions[0].item()\n",
    "            # Return position AFTER the EOT token (+0 includes EOT, +1 would go past it)\n",
    "            return eot_pos + 1  # Include EOT itself in loss\n",
    "        return len(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = SpecialTokenCollator(\n",
    "    assistant_header=\"<|start_header_id|>assistant<|end_header_id|>\",\n",
    "    eot_token=\"<|eot_id|>\",\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IGNORE: '<|begin_of_text|>'\n",
      "IGNORE: '<|start_header_id|>'\n",
      "IGNORE: 'assistant'\n",
      "IGNORE: '<|end_header_id|>'\n",
      "TRAIN: '\\n'\n",
      "TRAIN: 'response'\n",
      "TRAIN: ' text'\n",
      "TRAIN: '\\n'\n",
      "TRAIN: '<|eot_id|>'\n"
     ]
    }
   ],
   "source": [
    "input_text = \"\"\"<|start_header_id|>assistant<|end_header_id|>\n",
    "response text\n",
    "<|eot_id|>\"\"\"\n",
    "\n",
    "sample = tokenizer(input_text, return_tensors=\"pt\")\n",
    "collated = collator([{\"input_ids\": sample[\"input_ids\"][0]}])\n",
    "\n",
    "for token, label in zip(sample[\"input_ids\"][0], collated[\"labels\"][0]):\n",
    "    decoded = tokenizer.decode(token)\n",
    "    status = \"TRAIN\" if label != -100 else \"IGNORE\"\n",
    "    print(f\"{status}: {repr(decoded)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "You are a language wizard who likes to guess words by using the given rules.\n",
      "\n",
      "Welcome to Wordle! You have six attempts to guess the target word, a valid English word of five lowercase letters (a-z). Please use the tags \"guess:\" and \"explanation:\" to provide a concise explanation for each guess.\n",
      "\n",
      "To help you make an informed guess, you will receive a clue for the word, such as\n",
      "clue: \"snowy white\"\n",
      "\n",
      "Here is an example guess based on the clue:\n",
      "guess: apple\n",
      "explanation: In the fairy tail Snow White, the girl is killed because she eats a poisoned apple. And the word apple has 5 letters.\n",
      "\n",
      "I will then indicate whether I agree or disagree with your guess and provide rationale, but agreeing with a guess does not confirm its correctness. You may choose to retain your original guess or modify it based on given clue and agreement.\n",
      "\n",
      "After each guess, your answer will be validated, and you will receive feedback indicating which letters are correct (green), which letters are correct but in the wrong position (yellow), and which letters are incorrect (red). This feedback can be useful in determining which letters to include or exclude in your next guess.\n",
      "\n",
      "For example, the feedback for \"apple\" might be:\n",
      "guess_feedback: a<yellow> p<yellow> p<green> l<yellow> e<red>\n",
      "\n",
      "Here are some rules to keep in mind:\n",
      "1. Please do not generate any guess feedback or clue on your own\n",
      "2. Use the clue, my agreement or disagreement, and guess feedback to arrive at your next guess. This should be reflected in your explanation as well.\n",
      "\n",
      "Let's begin with your first guess.\n",
      "\n",
      "clue:bronze place<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "guess: third\n",
      "explanation: The bronze medal is awarded to the third-place finisher in competitions, so \"third\" aligns with the clue \"bronze place\" and is a five-letter word.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "clue:bronze place\n",
      "guess_agreement:yes\n",
      "agreement_explanation:your guess \"third\" fits the clue \"bronze place\" because the bronze medal is awarded to the third-place finisher, and \"third\" is a five-letter word.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "guess: medal\n",
      "explanation: Although \"third\" fits the clue \"bronze place\" and you agreed with it, another five-letter word associated with bronze awards is \"medal.\" Since bronze medals are given to third-place finishers, \"medal\" also aligns with the clue. I'll guess \"medal\" this time.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "guess_feedback: m<red> e<red> d<yellow> a<red> l<red><|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "guess: third\n",
      "explanation: Based on the clue \"bronze place\" and your agreement with my initial guess \"third,\" I believe \"third\" is still the best guess. From the feedback on \"medal,\" we know that:\n",
      "\n",
      "- The letter **D** is in the word but not in position 3 (since it was yellow when in position 3).\n",
      "- Letters **M**, **E**, **A**, and **L** are not in the word.\n",
      "\n",
      "In \"third,\" the **D** is in position 5, which fits the feedback. Also, all the letters in \"third\" are valid since they don't include the eliminated letters. Therefore, I'll guess \"third\" again.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "input_text = df.text.iloc[5]\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask: [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "# Test mask generation\n",
    "sample = tokenizer(input_text, return_tensors=\"pt\")\n",
    "input_ids = sample[\"input_ids\"][0]\n",
    "collated = collator([{\"input_ids\": input_ids}])\n",
    "mask = (collated[\"labels\"][0] != -100).tolist()\n",
    "print(\"Mask:\", mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training tokens: \n",
      "\n",
      "guess: third\n",
      "explanation: The bronze medal is awarded to the third-place finisher in competitions, so \"third\" aligns with the clue \"bronze place\" and is a five-letter word.<|eot_id|>\n",
      "\n",
      "guess: medal\n",
      "explanation: Although \"third\" fits the clue \"bronze place\" and you agreed with it, another five-letter word associated with bronze awards is \"medal.\" Since bronze medals are given to third-place finishers, \"medal\" also aligns with the clue. I'll guess \"medal\" this time.<|eot_id|>\n",
      "\n",
      "guess: third\n",
      "explanation: Based on the clue \"bronze place\" and your agreement with my initial guess \"third,\" I believe \"third\" is still the best guess. From the feedback on \"medal,\" we know that:\n",
      "\n",
      "- The letter **D** is in the word but not in position 3 (since it was yellow when in position 3).\n",
      "- Letters **M**, **E**, **A**, and **L** are not in the word.\n",
      "\n",
      "In \"third,\" the **D** is in position 5, which fits the feedback. Also, all the letters in \"third\" are valid since they don't include the eliminated letters. Therefore, I'll guess \"third\" again.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# Or get only the masked tokens:\n",
    "numeric_mask = [i for i, mm in enumerate(mask) if mm]\n",
    "masked_tokens = input_ids[numeric_mask]\n",
    "print(\"\\nTraining tokens:\", tokenizer.decode(masked_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tokenizer.eos_token_id == tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify EOT is single token\n",
    "assert len(tokenizer.encode(\"<|eot_id|>\", add_special_tokens=False)) == 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
