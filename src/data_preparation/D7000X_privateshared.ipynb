{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T11:27:43.041905Z",
     "start_time": "2024-12-23T11:27:42.293782Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import copy\n",
    "#from src.utils.utils import ensure_alternating_roles\n",
    "\n",
    "path: str = ('../../data/processed/privateshared_old_processed.jsonl')\n",
    "data = pd.read_json(path, lines=True)"
   ],
   "outputs": [],
   "execution_count": 236
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T11:27:55.184896Z",
     "start_time": "2024-12-23T11:27:55.144484Z"
    }
   },
   "cell_type": "code",
   "source": "data.benchmark_version.unique()",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['v0.9', 'v1.0'], dtype=object)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 237
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T13:54:34.033967Z",
     "start_time": "2024-12-22T13:54:34.019168Z"
    }
   },
   "source": [
    "def ensure_alternating_roles(messages, cull_system_message: bool = True):\n",
    "    \"\"\"\n",
    "    The messages format assumes alternating roles of user and assistant. This method checks, if this constraint\n",
    "    is satisfied. If this is not the case and there are consecutive user or assistant messages,\n",
    "    then these are merged into a single one.\n",
    "\n",
    "    :param cull_system_message:\n",
    "    :param messages: to be checked\n",
    "    :return: a new messages object with the alternating roles ensured\n",
    "    \"\"\"\n",
    "    consec_msg = 0\n",
    "    _messages = copy.deepcopy(messages)\n",
    "\n",
    "    if cull_system_message:\n",
    "        if _messages[0]['role'] == \"system\" and not _messages[0][\"content\"]:\n",
    "            del _messages[0]\n",
    "\n",
    "    def is_same_role(msg1, msg2):\n",
    "        return msg1[\"role\"] == msg2[\"role\"]\n",
    "\n",
    "    delimiter = \"\\n\\n\"\n",
    "\n",
    "    def join_content(msg1, msg2):\n",
    "        return f\"{msg1['content']}{delimiter}{msg2['content']}\"\n",
    "\n",
    "    if len(_messages) <= 1:\n",
    "        return _messages\n",
    "\n",
    "    def is_valid(idx):\n",
    "        return idx < len(_messages)\n",
    "\n",
    "    msg_idx = 1\n",
    "    while is_valid(msg_idx):\n",
    "        prev_message = _messages[msg_idx - 1]\n",
    "        message = _messages[msg_idx]\n",
    "        if is_same_role(prev_message, message):\n",
    "            warn_msg = (f\"Found consecutive role assignments. These will be merged into one:\\n\"\n",
    "                        f\"{prev_message}\\n\"\n",
    "                        f\"{message}\")\n",
    "            #logger.warning(warn_msg)\n",
    "            #print(\"X\")\n",
    "            consec_msg += 1\n",
    "            prev_message['content'] = join_content(prev_message, message)\n",
    "            del _messages[msg_idx]\n",
    "        else:\n",
    "            msg_idx += 1\n",
    "    #print(f\"{consec_msg} consecutive messages have been merged!\")\n",
    "    return _messages\n"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T13:56:41.420485Z",
     "start_time": "2024-12-22T13:56:41.413383Z"
    }
   },
   "source": [
    "def prepare_qa_data_psh1(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Splits privateshared dialogues in such a way that all probing blocks remain together.\n",
    "    \"\"\"\n",
    "    result_data: pd.DataFrame = {key: [] for key in data.columns}\n",
    "\n",
    "    for i, row in data.iterrows():\n",
    "        chat = row['chat']\n",
    "        game = row['game']\n",
    "\n",
    "        chat_last_index = len(chat)-1\n",
    "\n",
    "        for i, item in enumerate(chat):\n",
    "            if item['type'] == 'send message':\n",
    "                for column in data.columns:\n",
    "                    if chat[:i]:\n",
    "                        if column == 'chat':\n",
    "                            result_data[column].append(chat[:i]) # append everything above \"send message\"\n",
    "                        else: \n",
    "                            result_data[column].append(row[column])\n",
    "                         \n",
    "            else:\n",
    "                if i == chat_last_index: # maybe should test whether last entry is also assistant?\n",
    "                    for key in data.columns:\n",
    "                            result_data[key].append(row[key])\n",
    "    return result_data"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_qa_data_psh2(data: pd.DataFrame) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Splits privateshared dialogues in such a way that only a sample of each probing block remains together.\n",
    "#     \"\"\"\n",
    "#     result_data: pd.DataFrame = {key: [] for key in data.columns}\n",
    "\n",
    "#     for i, row in data.iterrows():\n",
    "#         chat = row['chat']\n",
    "#         chat_last_index = len(chat)-1\n",
    "#         messages = []\n",
    "#         probes = []\n",
    "#         for i, item in enumerate(chat):\n",
    "#             if \"message\" in item[\"type\"]:\n",
    "#                 messages.append(item)\n",
    "#             elif \"probe\" in item[\"type\"]:\n",
    "#                 if i != chat_last_index:\n",
    "#                     if \"probe\" in chat[i+1][\"type\"]: # if next turn is also part of probing\n",
    "#                         probes.append(item)\n",
    "#                     else:\n",
    "#                         probes.append(item)\n",
    "#                         for column in data.columns:\n",
    "#                             if column == 'chat':\n",
    "#                                 # sample probes here\n",
    "#                                 result_data[column].append(messages + probes) # append everything above \"send message\"\n",
    "#                                 probes = []\n",
    "#                             else: \n",
    "#                                 result_data[column].append(row[column])\n",
    "                            \n",
    "                        \n",
    "#                 else:\n",
    "#                     for column in data.columns:\n",
    "#                         probes.append(item)\n",
    "#                         if column == 'chat':\n",
    "#                             result_data[column].append(messages + probes) # append everything above \"send message\"\n",
    "#                         else: \n",
    "#                             result_data[column].append(row[column])\n",
    "\n",
    "\n",
    "#     return result_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_qa_data_psh3(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Splits privateshared dialogues in such a way that all probing blocks remain together and only one probing block remains.\n",
    "    \"\"\"\n",
    "    result_data: pd.DataFrame = {key: [] for key in data.columns}\n",
    "\n",
    "    for i, row in data.iterrows():\n",
    "        chat = row['chat']\n",
    "        chat_last_index = len(chat)-1\n",
    "        messages = []\n",
    "        probes = []\n",
    "        for i, item in enumerate(chat):\n",
    "            if \"message\" in item[\"type\"]:\n",
    "                messages.append(item)\n",
    "            elif \"probe\" in item[\"type\"]:\n",
    "                if i != chat_last_index:\n",
    "                    if \"probe\" in chat[i+1][\"type\"]: # if next turn is also part of probing\n",
    "                        probes.append(item)\n",
    "                    else:\n",
    "                        probes.append(item)\n",
    "                        for column in data.columns:\n",
    "                            if column == 'chat':\n",
    "                                result_data[column].append(messages + probes) # append everything above \"send message\"\n",
    "                                probes = []\n",
    "                            else: \n",
    "                                result_data[column].append(row[column])\n",
    "                            \n",
    "                        \n",
    "                else:\n",
    "                    for column in data.columns:\n",
    "                        \n",
    "                        if column == 'chat':\n",
    "                            probes.append(item)\n",
    "                            result_data[column].append(messages + probes) # append everything above \"send message\"\n",
    "                            probes = []\n",
    "                        else: \n",
    "                            result_data[column].append(row[column])\n",
    "\n",
    "    return result_data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T13:56:59.112990Z",
     "start_time": "2024-12-22T13:56:59.076421Z"
    }
   },
   "source": [
    "save_path: str = '../../data/training_data/D8000p1.csv'\n",
    "data_successful = data[data.Success == 1]\n",
    "print(len(data_successful))\n",
    "data_successful.chat = data_successful.chat.apply(ensure_alternating_roles)\n",
    "result = prepare_qa_data_psh1(data_successful)\n",
    "data_collapsed = pd.DataFrame(result)\n",
    "print(len(data), len(data_collapsed))\n",
    "#data_collapsed[[\"chat\"]].iloc[:15].to_csv(\"psh1.csv\")\n",
    "\n",
    "#ata_collapsed.to_csv(save_path, index=False)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "2497 336\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "2497 336\n"
     ]
    }
   ],
   "source": [
    "save_path: str = '../../data/training_data/D8000p3.csv'\n",
    "data_successful = data[data.Success == 1]\n",
    "print(len(data_successful))\n",
    "\n",
    "result = prepare_qa_data_psh3(data_successful)\n",
    "data_collapsed = pd.DataFrame(result)\n",
    "\n",
    "# important to merge *after* splitting so that the first probe question is not automatically appended to the prompt\n",
    "data_collapsed.chat = data_collapsed.chat.apply(ensure_alternating_roles)\n",
    "\n",
    "print(len(data), len(data_collapsed))\n",
    "#data_collapsed[[\"chat\"]].iloc[:33].to_csv(\"psh3_ear.csv\")\n",
    "\n",
    "data_collapsed.to_csv(save_path, index=False)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Rework of p1 -> D70001"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T17:38:04.118004Z",
     "start_time": "2024-12-22T17:38:04.109472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_probe_question_mask(chat: list) -> list:\n",
    "    probe_questions_indexes: list = []\n",
    "    is_probe: bool = False\n",
    "\n",
    "    for chat_idx, chat_item in enumerate(chat):\n",
    "        if chat_item['type'] == 'send message':\n",
    "            if is_probe:\n",
    "                is_probe = False\n",
    "        if chat_item['type'] == 'probe question':\n",
    "            if not is_probe:\n",
    "                is_probe = True\n",
    "            probe_questions_indexes.append(chat_idx)\n",
    "        if chat_item['type'] == 'probe answer':\n",
    "            probe_questions_indexes.append(chat_idx)\n",
    "\n",
    "    return probe_questions_indexes\n",
    "\n",
    "def prepare_qa_data_psh4(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    result_data: pd.DataFrame = {key: [] for key in data.columns}\n",
    "    \n",
    "    for idx, row in data.iterrows():\n",
    "        chat = row['chat']\n",
    "        probe_question_mask = get_probe_question_mask(chat)\n",
    "\n",
    "        chat_last_index = len(chat)\n",
    "\n",
    "        for i, item in enumerate(chat):\n",
    "            if i in probe_question_mask:\n",
    "                \n",
    "                # check that we are at the end of a probe block\n",
    "                if i+1 not in probe_question_mask:\n",
    "                    for key in data.columns:\n",
    "                        if key != 'chat':\n",
    "                            result_data[key].append(row[key])\n",
    "                        else:\n",
    "                            if i + 1 < chat_last_index:\n",
    "                                result_data[key].append(chat[:i + 1])\n",
    "                            else:\n",
    "                                result_data[key].append(row[key])     \n",
    "                continue\n",
    "                \n",
    "            if item['role'] == 'assistant':\n",
    "                for key in data.columns:\n",
    "                    if key != 'chat':\n",
    "                        result_data[key].append(row[key])\n",
    "                    else:\n",
    "                        if i + 1 < chat_last_index:\n",
    "                            result_data[key].append(chat[:i + 1])\n",
    "                        else:\n",
    "                            result_data[key].append(row[key])\n",
    "    return result_data"
   ],
   "outputs": [],
   "execution_count": 171
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T17:38:04.833650Z",
     "start_time": "2024-12-22T17:38:04.709850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_successful = data[data.Success == 1]\n",
    "print(len(data_successful))\n",
    "\n",
    "result = prepare_qa_data_psh4(data_successful)\n",
    "data_collapsed_ph4 = pd.DataFrame(result)\n",
    "\n",
    "# important to merge *after* splitting so that the first probe question is not automatically appended to the prompt\n",
    "data_collapsed_ph4.chat = data_collapsed_ph4.chat.apply(ensure_alternating_roles)\n",
    "\n",
    "print(len(data), len(data_collapsed_ph4))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "2497 631\n"
     ]
    }
   ],
   "execution_count": 172
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T17:38:07.235980Z",
     "start_time": "2024-12-22T17:38:07.107587Z"
    }
   },
   "cell_type": "code",
   "source": [
    "save_path: str = '../../data/training_data/D70001.csv'\n",
    "data_collapsed_ph4.to_csv(save_path, index=False)"
   ],
   "outputs": [],
   "execution_count": 173
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# P70002"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T17:50:36.459779Z",
     "start_time": "2024-12-22T17:50:36.450160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "def get_probe_question_mask(chat: list) -> (list, list):\n",
    "    probe_questions_indexes: list = []\n",
    "    probe_mask: list = []\n",
    "    is_probe: bool = False\n",
    "\n",
    "    for chat_idx, chat_item in enumerate(chat):\n",
    "        if chat_item['type'] == 'send message':\n",
    "            if is_probe:\n",
    "                is_probe = False\n",
    "        if chat_item['type'] == 'probe question':\n",
    "            if not is_probe:\n",
    "                is_probe = True\n",
    "            probe_questions_indexes.append(chat_idx)\n",
    "            probe_mask.append(chat_idx)\n",
    "        if chat_item['type'] == 'probe answer':\n",
    "            probe_mask.append(chat_idx)\n",
    "            \n",
    "    return probe_questions_indexes, probe_mask\n",
    "\n",
    "def sample(idx_list: list, fraction: float) -> list:\n",
    "    num_samples: int = int(len(idx_list) * (fraction/2))\n",
    "    new_samples = random.sample(idx_list, num_samples)\n",
    "    \n",
    "    return_samples = []\n",
    "    \n",
    "    for n in new_samples:\n",
    "        if n%2 == 0:\n",
    "            return_samples.append(n)\n",
    "            return_samples.append(n-1)\n",
    "        else:\n",
    "            return_samples.append(n)\n",
    "            return_samples.append(n+1)\n",
    "    \n",
    "    return list(set(return_samples))\n",
    "            \n",
    "\n",
    "def prepare_qa_data_psh2(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    result_data: pd.DataFrame = {key: [] for key in data.columns}\n",
    "\n",
    "    for idx, row in data.iterrows():\n",
    "        chat = row['chat']\n",
    "        probe_question_mask, _ = get_probe_question_mask(chat)\n",
    "        probe_questions_to_drop = sample(probe_question_mask, 0.9)\n",
    "        \n",
    "        reduced_chat = [c for i, c in enumerate(chat) if i not in probe_questions_to_drop]\n",
    "        \n",
    "        _, probe_mask = get_probe_question_mask(reduced_chat)\n",
    "        chat_last_index = len(reduced_chat)\n",
    "\n",
    "        for i, item in enumerate(reduced_chat):\n",
    "            if i in probe_mask:\n",
    "\n",
    "                # check that we are at the end of a probe block\n",
    "                if i+1 not in probe_mask:\n",
    "                    for key in data.columns:\n",
    "                        if key != 'chat':\n",
    "                            result_data[key].append(row[key])\n",
    "                        else:\n",
    "                            if i + 1 < chat_last_index:\n",
    "                                result_data[key].append(reduced_chat[:i + 1])\n",
    "                            else:\n",
    "                                result_data[key].append(row[key])\n",
    "                continue\n",
    "\n",
    "            if item['role'] == 'assistant':\n",
    "                for key in data.columns:\n",
    "                    if key != 'chat':\n",
    "                        result_data[key].append(row[key])\n",
    "                    else:\n",
    "                        if i + 1 < chat_last_index:\n",
    "                            result_data[key].append(reduced_chat[:i + 1])\n",
    "                        else:\n",
    "                            result_data[key].append(row[key])\n",
    "    return result_data"
   ],
   "outputs": [],
   "execution_count": 229
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T17:50:36.996320Z",
     "start_time": "2024-12-22T17:50:36.901690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_successful = data[data.Success == 1]\n",
    "print(len(data_successful))\n",
    "\n",
    "result = prepare_qa_data_psh2(data_successful)\n",
    "data_collapsed_ph2 = pd.DataFrame(result)\n",
    "\n",
    "# important to merge *after* splitting so that the first probe question is not automatically appended to the prompt\n",
    "data_collapsed_ph2.chat = data_collapsed_ph2.chat.apply(ensure_alternating_roles)\n",
    "\n",
    "print(len(data_collapsed_ph4), len(data_collapsed_ph2))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "631 629\n"
     ]
    }
   ],
   "execution_count": 230
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T17:50:37.014719Z",
     "start_time": "2024-12-22T17:50:36.998001Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lenC = 0\n",
    "for i, r in data_collapsed_ph4.iterrows():\n",
    "    lenC += len(r['chat'])"
   ],
   "outputs": [],
   "execution_count": 231
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T17:50:37.294874Z",
     "start_time": "2024-12-22T17:50:37.291942Z"
    }
   },
   "cell_type": "code",
   "source": "lenC ",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63780"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 232
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T17:50:37.583028Z",
     "start_time": "2024-12-22T17:50:37.572778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lenB = 0\n",
    "for i, r in data_collapsed_ph2.iterrows():\n",
    "    lenB += len(r['chat'])"
   ],
   "outputs": [],
   "execution_count": 233
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T17:50:38.137468Z",
     "start_time": "2024-12-22T17:50:38.135022Z"
    }
   },
   "cell_type": "code",
   "source": "lenB",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39842"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 234
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T17:51:02.041128Z",
     "start_time": "2024-12-22T17:51:01.944011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "save_path: str = '../../data/training_data/D70002.csv'\n",
    "data_collapsed_ph2.to_csv(save_path, index=False)"
   ],
   "outputs": [],
   "execution_count": 235
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
